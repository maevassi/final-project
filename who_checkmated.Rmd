---
title: "Who Checkmated?"
subtitle: "Predicting the winner of a chess game"
author: "Maeva Assi"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Packages
library(tidyverse)
library(ggplot2)
library(openintro)
library(caret)
library(rpart)
library(rpart.plot)
library(pROC)
library(randomForest)

# Downloads the picture of a chess board
figure_filename1 <- "checkmate.png"

figure_url1 <- "https://www.tapsmart.com/wp-content/uploads/2020/12/chess-header.jpg"
download.file(
  url = figure_url1,
  destfile = figure_filename1,
  mode = "wb"
)

# Downloads the picture of a chess board
figure_filename2 <- "chess_board_header.png"
figure_url2 <- "https://chessbazaar.gumlet.io/media/catalog/product/y/y/yy.jpg"
download.file(
  url = figure_url2,
  destfile = figure_filename2,
  mode = "wb"
)

```

# Introduction:



In chess, the player with the light-colored pieces is referred to as "White" and the player with the dark-colored pieces is referred to as "Black". White moves first, after which players alternate turns.\
Each player starts with sixteen pieces: one King, one Queen, two Rooks, two Knights, two Bishops, and eight Pawns. The main point of chess is to **checkmate the opponent's King**.\

**Checkmate** (often shortened to mate) means **attacking the King so that it cannot escape capture**, thus ending the game.\
The King is never actually captured – a player loses as soon as their King is checkmated.\
A game also ends if a player resigns or forfeits, or if time runs out for one of the players but today's article will focus on games ending in **checkmates** only.\
![*A chess board.*](chess_board_header.png)\


The [Chess Game Dataset](https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-10-01/readme.md "Chess Game Dataset via GitHub") is a set of just over 20,000 games collected from a selection of users from the free open source chess server **Lichess.org** via [Kaggle by Mitchell J.](https://www.kaggle.com/datasets/datasnaek/chess/data "Chess Game Dataset via Kaggle by Mitchell J.").\
The set contains the 16 following variables for each game:

- `game_id`: Game ID;
- `rated`:	Rated (T/F), which is whether the game was a rated game (which will affect your rating in the platform), or a "casual" game.
- `start_time`: Start time;
- `end_time`: End time;
- `turns`: Number of turns;
- `victory_status`: Game status (mate, resign, draw, out of time);
- `winner`: Winner (white, black, draw);
- `time_increment`: Time increment;
- `white_id`:	White player id;
- `white_rating`:	White player rating;
- `black_id`:	Black player id;
- `black_rating`:	Black player rating;
- `moves`: All Moves in Standard Chess Notation;
- `opening_eco`: Opening Eco (Standardised Code for any given opening, list [here](https://www.365chess.com/eco.php))
- `opening_name`: Opening Name; and
- `opening_ply`: Number of moves in the opening phase.

Because chess is an abstract strategy game which involves no hidden information and no elements of chance, I'm curious as to what allows a player to win as black or white.
**Question**: Is there a relationship between player level and victory by checkmate? Can we predict if a game resulted in a black or white winner depending on the player level?


# I. Data Import and Wrangling
I imported the data directly from GitHub.
```{r message=FALSE, warning=FALSE}
# Importing the data
chess <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2024/2024-10-01/chess.csv')

# Looking at the data
glimpse(chess)
```

As stated in the introduction, the dataset has 16 variables, however I only need the following for my project:

- `rated`:	Rated (T/F), which is whether the game was a rated game (which will affect your rating in the platform), or a casual game.
- `victory_status`: Game status (mate, resign, draw, out of time);
- `winner`: Winner (white, black, draw);
- `white_rating`:	White player rating; and
- `black_rating`:	Black player rating.

I'm also only interested in games ending in **checkmates**, so I filtered for those and then select my variables of interest.\
I didn't select the `victory_status` variable since it only takes "mate" as a value after filtering, making it redundant.\
As a reminder of the game rules, a checkmate cannot be a draw as it must result in a white or black winner, so I didn't need to filter out "draw" from the `winner` variable after filtering the `victory_status`.
```{r}
chess_clean <- chess |>
  # Filtering for the relevant victory status
  filter(victory_status == "mate"
         ) |>
  # Selecting the variables of interest
  select(rated,
         winner,
         white_rating,
         black_rating
         )

glimpse(chess_clean)
```
Filtering narrowed my data down to 6,325 games.

Next, since both the white and black player ratings are reported for each game, I created a variable that reports the rating of the winning player using a logical vector. This variable will be useful when doing data visualizations.
```{r}
# Creating a new variable that reports the rating of the winner
chess_clean <- chess_clean |>
  mutate(winner_rating = if_else(winner == "white", # condition: if White is the winner
                          white_rating, # if true, winner rating = white player rating
                          black_rating # if false, winner rating = black player rating
                          ),
         .after = winner
  ) |>
  mutate(loser_rating = if_else(winner != "white", # condition: if White is not the winner
                          white_rating, # if true, loser rating = white player rating
                          black_rating # if false, loser rating = black player rating
                          ),
         .after = winner_rating
  ) |>
  mutate(rating_difference = winner_rating - loser_rating,
         .after = winner_rating
  )  |>
  mutate(white_black_difference = white_rating - black_rating,
         .after = winner_rating
  )

glimpse(chess_clean)
```
\
Lastly, because the `rated` variable name and its values may be confusing at a first glance, I renamed the `rated` variable to `game_type`, converted it into a factor and renamed its levels. This will make my data visualizations easier to understand.\
I also converted the `winner` variable into a factor for modeling.
```{r}
chess_clean <- chess_clean |>
  # Renaming the variable
  rename("game_type" = rated) |>
  # Converting variables to factors
  mutate(game_type = factor(game_type),
         winner = factor(winner)
         ) |>
  # Renaming the game type levels
  mutate(game_type =  fct_recode(game_type,
                                 "Rated Game" = "TRUE",
                                 "Casual Game" = "FALSE"
                                 )
         ) |>
  # Releveling the winner levels
  mutate(winner =  fct_relevel(winner,
                                 "white",
                                 "black"
                               )
         )
  
glimpse(chess_clean)
```

## Exploring the Data
I also created a separate object for summary statistics of player_rating by winner.
```{r message=FALSE, warning=FALSE}
# Selecting the variables of interest
chess_avg <- chess_clean |>
  summarize(winner_n = n(),
            avg_diff = mean(winner_rating) - mean(loser_rating),
            avg_winner_rating = mean(winner_rating),
            median_winner_rating = median(winner_rating),
            max_winner_rating = max(winner_rating),
            max_winner_rating = max(winner_rating),
            min_winner_rating = min(winner_rating),
            .by = winner)
chess_avg

# Selecting the variables of interest
chess_summary2 <- chess_clean |>
  summarize(winner_n = n(),
            diff_rating = white_rating - black_rating,
            .by = c(winner, winner_rating))
chess_summary2
```
Player ratings range from 784 for white winners and 796 for black winners to 2,621 for both.
The average player rating for black winners is 1,585.97, which is higher than the average player rating for white winners of 1,576.5 by `r round(1585.973-1576.498, digits = 2)`.

6 of the top 10 players with the highest player rating won as white players.
```{r}
chess_clean |>
  slice_max(winner_rating, n = 10)
```


# II. Data visualization:
First, I'd like to explore the possible differences between white and black winners.\
Based on the data, most checkmates were delivered by **white players** (3,344 versus 2981 for checkmates delivered by black players). They represent **`r round(3344/6325*100, digits = 2)`%** of all victories by checkmate out of 6,325 games.
```{r echo=FALSE, message=TRUE}
chess_clean |>
  ggplot(
    aes(x = (winner),
        fill = winner
        )
    ) +
  scale_fill_manual(values = c("lightyellow","antiquewhite4")) +
  geom_bar(colour = "black") +
  labs(
    title = "Distribution of Winners by Checkmate",
    x = "Winner",
    y = "Number of winners",
    fill = "Winner"
    )
```
\
I'm interested in whether player rating could impact the winning results. A boxplot of the winner rating by their chess color shows there is little difference between the player rating of white vs black winners.

```{r echo=FALSE, message=FALSE, warning=FALSE}
chess_clean |>
  ggplot(
    aes(x = winner,
        y = winner_rating,
        fill = winner
        )
    ) +
  scale_fill_manual(values = c("lightyellow","antiquewhite4")) +
  geom_boxplot() +
  labs(
    title = "Player rating of white vs black winners",
    x = "Winner",
    y = "Player Rating",
    fill = "Winner"
    )
```
\
Black winners have a median player rating of 1556, slightly higher than white winners 1544.5. Both have low outliers, at 784 for white winners and 786 for black winners, and high outliers up to 2,621.\
Some of these high outliers are caused by the same players, though white winners have a higher number of high outliers, which indicates that some of the players with the highest ratings won more as white than as black.

However when taking into account whether the game was a casual game or a rated game, there is a difference between white or black winners.

```{r echo=FALSE, message=FALSE, warning=FALSE}
chess_clean |>
  ggplot(
    aes(x = game_type,
        y = winner_rating,
        fill = winner
        )
    ) +
  scale_fill_manual(values = c("lightyellow","antiquewhite4")) +
  geom_boxplot() +
  labs(
    title = "Winner Rating by Chess color",
    x = "Game Type",
    y = "Winner Rating",
    fill = "Winner"
    )
```

White players have slightly more wins when they're underleveled compared to their black opponent.
```{r echo=FALSE, message=FALSE, warning=FALSE}
chess_clean |>
  ggplot(
    aes(x = winner,
        y = rating_difference,
        fill = winner
        )
    ) +
  scale_fill_manual(values = c("lightyellow","antiquewhite4")) +
  geom_boxplot() +
  coord_cartesian(ylim = c(-1000,2000)) +
  labs(
    title = "Winner Rating by Chess color",
    x = "Winner",
    y = "Difference between Winner & Loser Rating",
    fill = "Winner"
    )
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
chess_clean |> 
  filter(game_type == "Rated Game") |> 
  ggplot(
    mapping = aes(x = white_rating,
                  y = black_rating
                  )
    ) +
  geom_point(aes(color = winner,
                 shape = winner
                 )
             ) +
  scale_color_manual(values = c("lightyellow","antiquewhite4")) +
  geom_smooth(method = "lm") +
  labs(
    title = "Winners by Black vs White rating",
    x = "White player rating",
    y = "Black player rating"
    )
```

```{r}
chess_summary2
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
chess_clean |> 
  filter(game_type == "Rated Game") |> 
  ggplot(
    mapping = aes(x = winner_rating,
                  y = rating_difference
                  )
    ) +
  geom_point(aes(color = winner,
                 shape = winner
                 )
             ) +
  scale_color_manual(values = c("lightyellow","antiquewhite4")) +
  geom_smooth(method = "lm") +
  labs(
    title = "Winners by Black vs White rating",
    x = "White player rating",
    y = "Black player rating"
    )
```

# III. Modeling/analysis:
Now that we see that may be some association between victory and player rating, let's model the data.

You could always create 2 models: 1 with the variables and 1 without and then compare which model has better perfomance.
# 1 You might want to try a random forest model (discussed on Thursday) since they are less likely to overfit than decision trees.

First, I split my data into 80% training and 20% testing data.

```{r}
# To ensure reproducibility
set.seed(2000)

# Split data into training and test dataset
train_indexes <- as.vector(createDataPartition(chess_clean$winner, p = 0.8, list = FALSE))
chess_train <- slice(chess_clean, train_indexes)
chess_test <- slice(chess_clean, -train_indexes)
```

## Decision trees
To predict if a game with a certain number of turns OR with a specific first move had a white or black winner.


We will now fit several classification trees to predict if a game ends in a white winner.

- Decision tree 1: fit a model using the `black player rating` as a predictor variable.
- Decision tree 2: fit a model using the `white player rating` as a predictor variable
- Decision tree 3: fit a model using both `white_rating` and `black_rating`as predictor variables.

```{r message=FALSE, warning=FALSE, include=FALSE}
# Fitting classification trees
remote_tree1 <- rpart(
  winner ~ black_rating,
  data = chess_train,
  method = "class"
  )

remote_tree2 <- rpart(
  winner ~ white_rating,
  data = chess_train,
  method = "class"
  )

remote_tree3 <- rpart(
  winner ~ white_rating + black_rating, 
  data = chess_train,
  method = "class"
  )
```

### Decision tree 1: `black player rating` as a predictor variable.
```{r fig.width=9, include=FALSE}
rpart.plot(
  remote_tree1,
  type = 5,
  extra = 104,
  fallen.leaves = TRUE,
  box.palette = "auto"
)
```

### Decision tree 2: `black player rating` as a predictor variable.
```{r fig.width=9, include=FALSE}
rpart.plot(
  remote_tree2,
  type = 5,
  extra = 104,
  fallen.leaves = TRUE,
  box.palette = "auto"
)
```

### Decision tree 3: both `white_rating` and `black_rating`as predictor variables.
```{r eval=FALSE, fig.width=9, include=FALSE}
rpart.plot(
  remote_tree3,
  type = 5,
  extra = 104,
  fallen.leaves = TRUE,
  box.palette = "auto"
)
```


Predicting the winner on the test set using each decision tree, calculating the ROC curve for each decision tree and plot them gives us the following figure:

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Predicing the winner on the test set using each decision tree
predicted_probs1 <- predict(remote_tree1, newdata = chess_test, type = "prob")
predicted_probs2 <- predict(remote_tree2, newdata = chess_test, type = "prob")
predicted_probs3 <- predict(remote_tree3, newdata = chess_test, type = "prob")

# Calculating the ROC curve
roc_obj1 <- roc(chess_test$winner, predicted_probs1[, "white"])
roc_obj2 <- roc(chess_test$winner, predicted_probs2[, "white"])
roc_obj3 <- roc(chess_test$winner, predicted_probs3[, "white"])

# Plotting ROC curve
plot.roc(roc_obj1,
         col = "antiquewhite4",
         lwd = 2,
         main = "ROC curve for decision trees",
         print.thres = "all",
         print.auc = TRUE,
         print.thres.cex = 0.7)
plot.roc(roc_obj2,
         col = "burlywood1",
         lwd = 2,
         add = TRUE)
plot.roc(roc_obj3,
         col = "pink",
         lwd = 2,
         add = TRUE)
legend("bottomright",
  legend = c(
    paste("Decisition tree 1 AUC =", round(auc(roc_obj1), 2)),
    paste("Decisition tree 2 AUC =", round(auc(roc_obj2), 2)),
    paste("Decisition tree 3 AUC =", round(auc(roc_obj3), 2))
    ),
 col = c("antiquewhite4", "burlywood1", "pink"),
 lwd = 2
 )
```

The third decision tree using both `white_rating` and `black_rating`as predictor variables has the highest area under the curve of `r auc(roc_obj3)`.

##  Random forest model

```{r message=FALSE, warning=FALSE}
set.seed(444)
# Select features and fit the model
rf_model <- randomForest(winner ~ ., data = chess_train)

# Inspect the model
rf_model 
```

The out-of-bag error estimate was 2.23%. This means that `r 100-2.23`% of the out-of-bag observations were classified correctly.\

● 2,628 white winners were correctly labeled as white (These are true negatives.)
● 48 white winners were incorrectly labeled as black (This is a false positive.)
● 65 black winners were incorrect labeled as white. (This is a false negative.)
● 2,320 black winners were correctly labeled as black (These are true positives.)

```{r include=FALSE}
oob_err_df <- as_tibble(rf_model$err.rate) |>
  # Add a number of trees variable (this is simply the row number)
  mutate(num_trees = row_number()) |>
  # Convert to long format
  pivot_longer(cols = -num_trees, names_to = "error_type", values_to =
"error_rate") |>
  mutate(
    # Use human readable labels
    error_type = fct_recode(error_type,
      "Overall" = "OOB",
      "White winner" = "white",
      "Black winner" = "black"
),
    # Calculate accuracy as well
    accuracy = 1 - error_rate
  )
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
oob_err_df |>
  mutate(error_type_ordered = fct_reorder2(error_type,
                                           num_trees,
                                           accuracy
                                           )
         ) |>
  ggplot(aes(x = num_trees,
             y = accuracy,
             color = error_type_ordered
             )
         ) +
    geom_line() +
    labs(title = "Out-of-bag accuracy as a function of the number of trees",
         x = "# of trees",
         y = "Out-of-bag accuracy",
         color = "Class") +
    theme_classic()
```
The error rate sort of stabilizes at around 200 trees.

```{r echo=FALSE, message=FALSE, warning=FALSE}
# Define tuning grid: try several values of mtry
tune_grid <- expand.grid(mtry = 1:10)

# Set up the training function
ctrl <- trainControl(method = "oob") # Use OOB estimate for training evaluation

# Fit the randomForest using different mtry values
set.seed(345)
rf_tuned <- train(
  winner ~ .,
  data = chess_train,
  method = "rf",
  trControl = ctrl,
  tuneGrid = tune_grid,
  ntree = 250 # Set this reasonably large since we are tuning mtry
)

print(rf_tuned)

plot(rf_tuned)
```
Using 9 features at each split gives the best out-of-bag accuracy.

```{r echo=FALSE, message=FALSE, warning=FALSE}
set.seed(5632)
rf_model_final <- randomForest(winner ~ .,
                               data = chess_train,
                               mtry = rf_tuned$finalModel$tuneValue$mtry,
                               ntree = 250)
# Calculate the predicted values
predicted_test <- predict(rf_model_final,
                          newdata = chess_test,
                          type = "response")
# Calculate the predicted probabilites
predicted_prob_test <- predict(rf_model_final,
                               newdata = chess_test, type="prob")
# Create the ROC object
roc_obj <- roc(response = chess_test$winner,
               predictor = predicted_prob_test[,2],
               levels = c("white", "black"))
# Plot the ROC curve
plot(roc_obj, print.thres = "best", print.auc = TRUE)
```
The random forest has a higher accuracy than the decision trees, with an area under the curve of 0.999

# Conclusion/discussion:
![*A chess board.*](checkmate.png)\
There is a positive relationship between player rating.
summarize your key findings.
Reflect on what your analysis revealed, any limitations, and possible next steps.
**Question/Goal**: Predict if a game resulted in a black or white winner depending on the opening move.


# References:

### Sources:

- Chess Game Dataset:
  - via GitHub: https://github.com/rfordatascience/tidytuesday/blob/main/data/2024/2024-10-01/readme.md;
  - via Kaggle by Mitchell J: https://www.kaggle.com/datasets/datasnaek/chess/data
- "Checkmate", Wikipedia: https://en.wikipedia.org/wiki/Checkmate

### Images:

- Chess board: <https://chessbazaar.gumlet.io/media/catalog/product/y/y/yy.jpg>
- White King knocking down Black King: <https://www.tapsmart.com/wp-content/uploads/2020/12/chess-header.jpg>